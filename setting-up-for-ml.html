<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Faisal Z. Qureshi">
  <meta name="dcterms.date" content="2021-05-19">
  <title>Setting docker for ML</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="notes-style.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">Setting docker for ML</h1>
<p class="subtitle">Accessing host GPUs</p>
<p class="author">Faisal Z. Qureshi</p>
<p class="institute">Ontario Tech University</p>
<p class="webpage"><a href="http://faculty.uoit.ca/qureshi">http://faculty.uoit.ca/qureshi</a></p>
<p class="date">May 19, 2021</p>
<hr/>
</header>
<p>Machine Learning (ML) requires access to serious computing hardware, most notably Graphical Processing Units (GPUs) capable of crunching vast amount of data in order to train complicated deep (learning) neural networks models. NVidia is currently the dominant GPU provider on the market. A large fraction of deep learning models rely upon NVidia GPUs. Before we setup a docker environment that has access to GPUs available on the host machine, lets ensure that the following works. To keep things simple, lets assume that you are using a Linux system.</p>
<ol type="1">
<li><a href="https://developer.nvidia.com/cuda-gpus">Nvidia GPU</a> hardware is correctly installed on your machine.</li>
<li>You have downloaded and installed the latest <a href="https://www.nvidia.com/Download/index.aspx">drivers</a> for your GPU/OS combination.</li>
<li>You have also installed the correct version of Nvidia <a href="https://developer.nvidia.com/cuda-toolkit">cuda toolkit</a>.</li>
<li>In addition, you have also installed <a href="https://docs.docker.com/engine/install/ubuntu/">Docker</a> and <a href="https://docs.docker.com/compose/install/">docker-compose</a>.</li>
</ol>
<p><strong>Which GPUs are available</strong></p>
<p>Use the following commands to see GPU hardware available on your machine.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nvidia-smi</span></code></pre></div>
<p><strong>Which version of cuda is installed</strong></p>
<p>Use the following command to check cuda compiler version installed on your machine.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nvcc <span class="at">--version</span></span></code></pre></div>
<p><strong>Which version of linux is this system</strong></p>
<p>Check linux version</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cat /etc/<span class="pp">*</span>release</span></code></pre></div>
<p>Alternately, you can use <code>lsb_release -d</code> command.</p>
<p>You can install lsb package if not already installed</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> apt-get <span class="at">-y</span> install lsb-core</span></code></pre></div>
<h1 id="nvidia-container-toolkit">Nvidia Container Toolkit</h1>
<p>Nvidia has provided <a href="https://github.com/NVIDIA/nvidia-docker">Nvidia Container Toolkit</a> package that allows GPUs to be made available within the docker container. Install it as follows</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo apt-get install <span class="at">-y</span> nvidia-docker2</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo systemctl restart docker</span></code></pre></div>
<h1 id="setting-up-nvidiacuda-container">Setting up Nvidia/cuda container</h1>
<p>Youâ€™re almost there.</p>
<h2 id="option-1">Option 1</h2>
<p>You can <code>docker pull</code> to get a pre-built container. Check <a href="https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html">here</a> and <a href="https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/unsupported-tags.md">here</a> for more information.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker pull nvidia/cuda:11.3.1-cudnn8-runtime-ubuntu20.04</span></code></pre></div>
<p>This will require <code>docker login</code>.</p>
<h2 id="option-2">Option 2</h2>
<p>This is my preferred way of doing things. Create a <code>Dockerfile</code> as follows. The first line is important and it specifies the base image nvidia/cuda image to build from. The rest is standard docker stuff as seen <a href="python-dev.html">here</a>.</p>
<p><strong>Dockerfile</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>ARG DEBIAN_FRONTEND=noninteractive</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>RUN apt-get update &amp;&amp; apt-get -y upgrade</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>RUN apt-get install -y build-essential python3 python3-pip python-dev sudo</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>RUN mkdir -p /tmp</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>COPY requirements.txt /tmp/</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>RUN pip3 -q install pip --upgrade</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>RUN pip3 install -r /tmp/requirements.txt -f https://download.pytorch.org/whl/torch_stable.html</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>RUN groupadd -g 1010 dockeruser</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>RUN useradd -r -m -g 1010 dockeruser</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>RUN chown -R dockeruser /home/dockeruser</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>RUN chmod -R g+rwx /home/dockeruser</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>USER dockeruser</span></code></pre></div>
<p>Since I am interested in <a href="https://pytorch.org">PyTorch</a>, my <code>requirements.txt</code> file contains the following.</p>
<p><strong>requirements.txt</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>torch==1.8.0+cu111</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>torchvision==0.9.0+cu111</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>matplotlib</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>jupyter</span></code></pre></div>
<p>Find out the appropriate torch package with the correct cuda support <a href="https://download.pytorch.org/whl/torch_stable.html">here</a>. This sort of needs to match with the cuda support available on the host and also the nvidia/cuda base image that you use as the base image (as seen in the docker file).</p>
<h3 id="building-your-own-nvidiacuda-docker-container">Building your own nvidia/cuda docker container</h3>
<p>Build docker container as usual.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker build <span class="at">-t</span> myml .</span></code></pre></div>
<h1 id="run-container">Run container</h1>
<p>You can run docker container as follows</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker run <span class="at">-it</span> <span class="at">--gpus</span><span class="op">=</span>all <span class="at">--ipc</span><span class="op">=</span>host myml bash</span></code></pre></div>
<p>From within you can see if PyTorch is able to access host GPUs as follows.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[Container]</span> $ python3</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Python</span> 3.6.9 <span class="er">(</span><span class="ex">default,</span> Jan 26 2021, 15:33:00<span class="kw">)</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="ex">[GCC</span> 8.4.0] on linux</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Type</span> <span class="st">&quot;help&quot;</span>, <span class="st">&quot;copyright&quot;</span>, <span class="st">&quot;credits&quot;</span> or <span class="st">&quot;license&quot;</span> for more information.</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> import <span class="ex">torch</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> torch.cuda.is_available<span class="kw">()</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="ex">True</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> torch.cuda.get_device_name<span class="kw">(</span><span class="ex">0</span><span class="kw">)</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="st">&#39;GeForce GTX TITAN X&#39;</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> torch.cuda.get_device_name<span class="kw">(</span><span class="ex">1</span><span class="kw">)</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="st">&#39;GeForce GTX 980&#39;</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span></code></pre></div>
<p><em>Happy hacking!</em></p>
</body>
</html>
